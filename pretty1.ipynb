{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pretty1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "import timeit\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import math\n",
        "\n",
        "import string\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder,MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, KFold\n",
        "from sklearn.metrics import *\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "! pip install -q catboost\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "import pickle,joblib\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ],
      "metadata": {
        "id": "JKoHk323kvnv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaedd552-9a6e-4c92-d761-788b045d5ce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_1(df):\n",
        "\n",
        "  # 1) D / H / L 코드 파생변수 생성  \n",
        "  D_code_1=D_code.rename(columns={'속성 D 코드':'person_prefer_d_1','속성 D 세분류코드':'person_prefer_d_1_m','속성 D 대분류코드':'person_prefer_d_1_l'}, inplace = False).drop(['속성 D 소분류코드','속성 D 중분류코드'],axis=\"columns\")\n",
        "  D_code_2=D_code.rename(columns={'속성 D 코드':'person_prefer_d_2','속성 D 세분류코드':'person_prefer_d_2_m','속성 D 대분류코드':'person_prefer_d_2_l'}, inplace = False).drop(['속성 D 소분류코드','속성 D 중분류코드'],axis=\"columns\")\n",
        "  D_code_3=D_code.rename(columns={'속성 D 코드':'person_prefer_d_3','속성 D 세분류코드':'person_prefer_d_3_m','속성 D 대분류코드':'person_prefer_d_3_l'}, inplace = False).drop(['속성 D 소분류코드','속성 D 중분류코드'],axis=\"columns\")\n",
        "  D_code_contents=D_code.rename(columns = {'속성 D 코드' : 'contents_attribute_d','속성 D 세분류코드':'contents_attribute_d_m','속성 D 대분류코드':'contents_attribute_d_l'}, inplace = False).drop(['속성 D 소분류코드','속성 D 중분류코드'],axis=\"columns\")\n",
        "  H_code_1=H_code.rename(columns={'속성 H 코드':'person_prefer_h_1','속성 H 중분류코드':'person_prefer_h_1_m','속성 H 대분류코드':'person_prefer_h_1_l'}, inplace = False)\n",
        "  H_code_2=H_code.rename(columns={'속성 H 코드':'person_prefer_h_2','속성 H 중분류코드':'person_prefer_h_2_m','속성 H 대분류코드':'person_prefer_h_2_l'}, inplace = False)\n",
        "  H_code_3=H_code.rename(columns={'속성 H 코드':'person_prefer_h_3','속성 H 중분류코드':'person_prefer_h_3_m','속성 H 대분류코드':'person_prefer_h_3_l'}, inplace = False)\n",
        "  H_code_contents=H_code.rename(columns={'속성 H 코드' : 'contents_attribute_h','속성 H 중분류코드':'contents_attribute_h_m','속성 H 대분류코드':'contents_attribute_h_l'}, inplace = False)\n",
        "  L_code_contents=L_code.rename(columns={'속성 L 코드' : 'contents_attribute_l','속성 L 세분류코드':'contents_attribute_l_m','속성 L 대분류코드':'contents_attribute_l_l'}, inplace = False).drop(['속성 L 소분류코드','속성 L 중분류코드'],axis=\"columns\")\n",
        "\n",
        "  df_list=[D_code_1,D_code_2,D_code_3,D_code_contents,\n",
        "          H_code_1,H_code_2,H_code_3,H_code_contents, L_code_contents]\n",
        "  df_column=[\"person_prefer_d_1\",\"person_prefer_d_2\",\"person_prefer_d_3\",\"contents_attribute_d\",\n",
        "            \"person_prefer_h_1\",\"person_prefer_h_2\",\"person_prefer_h_3\",\"contents_attribute_h\",\n",
        "            \"contents_attribute_l\"]\n",
        "\n",
        "  for i in range(0,len(df_column)):\n",
        "    df = pd.merge(df, df_list[i],on=df_column[i]) \n",
        "\n",
        "  df.loc[(df['person_prefer_d_1_m']==df['contents_attribute_d_m'])|(df['person_prefer_d_2_m']==df['contents_attribute_d_m'])| (df['person_prefer_d_3_m']==df['contents_attribute_d_m']),'d_m_match_123']=True\n",
        "  df.loc[(df['person_prefer_d_1_l']==df['contents_attribute_d_l'])|(df['person_prefer_d_2_l']==df['contents_attribute_d_l'])| (df['person_prefer_d_3_l']==df['contents_attribute_d_l']),'d_l_match_123']=True\n",
        "  df.loc[(df['person_prefer_d_1']==df['contents_attribute_d'])|(df['person_prefer_d_2']==df['contents_attribute_d'])| (df['person_prefer_d_3']==df['contents_attribute_d']),'d_s_match_123']=True\n",
        "  df.loc[(df['person_prefer_h_1_l']==df['contents_attribute_h_l'])|(df['person_prefer_h_2_l']==df['contents_attribute_h_l'])| (df['person_prefer_h_3_l']==df['contents_attribute_h_l']),'h_l_match_123']=True\n",
        "  df.loc[(df['person_prefer_h_1_m']==df['contents_attribute_h_m'])|(df['person_prefer_h_2_m']==df['contents_attribute_h_m'])| (df['person_prefer_h_3_m']==df['contents_attribute_h_m']),'h_m_match_123']=True\n",
        "  df.loc[(df['person_prefer_h_1']==df['contents_attribute_h'])|(df['person_prefer_h_2']==df['contents_attribute_h'])| (df['person_prefer_h_3']==df['contents_attribute_h']),'h_s_match_123']=True\n",
        "  \n",
        "  df = df.fillna(False).sort_values(by=['id'],axis=0) \n",
        "\n",
        "\n",
        "  # 2) 변수형 변환\n",
        "  df.loc[:, df.columns != 'contents_open_dt'] = \\\n",
        "      df.loc[:, df.columns != 'contents_open_dt'].astype('int') # boolean -> int\n",
        "  numeric_columns = ['contents_rn','person_rn','contents_attribute_j'\n",
        "                     ]\n",
        "  categorical_columns = list(df.columns.drop(numeric_columns))\n",
        "  df=df[categorical_columns].astype('category')\n",
        "\n",
        "  # 3) id list 저장 \n",
        "  id_list = list(df['id'])\n",
        "  df = df.sort_values(by=['id'],axis=0).set_index('id')\n",
        "\n",
        "\n",
        "  # 4) 불필요한 컬럼 삭제 \n",
        "  df.drop(['person_prefer_f','person_prefer_g','contents_open_dt'],\n",
        "          axis=\"columns\",inplace=True)\n",
        "  \n",
        "  return id_list, df  \n"
      ],
      "metadata": {
        "id": "-pN1O0F-ksls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 디렉토리 설정: \"[DACON] 잡케어 추천 알고리즘 경진대회\" 폴더로 이동\n",
        "path = \"/content/drive/MyDrive/\"\n",
        "os.chdir(path)\n",
        "\n",
        "# 파일 불러오기 \n",
        "train = pd.read_csv('Jobcare_data/train.csv')\n",
        "test = pd.read_csv('Jobcare_data/test.csv')\n",
        "D_code=pd.read_csv('Jobcare_data/속성_D_코드.csv')\n",
        "L_code=pd.read_csv('Jobcare_data/속성_L_코드.csv')\n",
        "H_code=pd.read_csv('Jobcare_data/속성_H_코드.csv')\n",
        "\n",
        "# 1타입 전처리 함수 적용하기 \n",
        "train1_idx, train1 = preprocessing_1(train)\n",
        "#train1.to_csv('train1.csv', header=True,index=False)\n",
        "\n",
        "test1_idx, test1 = preprocessing_1(test)  \n",
        "#test1.to_csv('test1.csv', header=True,index=False)"
      ],
      "metadata": {
        "id": "vz6K86FuyaXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OYy6sVh_hCZN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}